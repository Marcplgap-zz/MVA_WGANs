{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WASSERSTEIN GENERATIVE ADVERSARIAL NETWORK: SOCOFing DATA\n",
    "\n",
    "*Authored by Iulia-Maia Muresan and Marc GonzÃ¡lez i Planellas*\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook is devoted to the implementation of a WGAN to the SOCOFing dataset available on Kaggle:\n",
    "\n",
    "https://www.kaggle.com/ruizgara/socofing\n",
    "\n",
    "Which contains 6000 fingerprint scans from 600 african subjects, which we encode in greyscale. The pertinent exploration of how the SOCOFing (along with that of Fashion Mnist) is structured and how to retrieve a workable array to input in the WGAN is explored in the *image_processing.ipyinb* complementary file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from numpy import expand_dims\n",
    "from numpy import mean\n",
    "from numpy import ones\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from keras import backend\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image, ImageOps # maybe you need to -pip install pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main characteristics of the Wasserstein GAN is that the **Critic** (the equivalent of the Discriminator for GAN in the original paper of Goodfellow et al.) is allowed to have a range of real values instead of assigning a binary class (fake or real) to the sampled image. However, to facilitate convergence, this indicator must be bound to a small value. The authors of WGAN define it at the $w \\in \\{-0.01,0.01\\}$ range, but given the difference in complexity due to the fingerprint patterns, we decide for a more strict constraint (see Kernel [4]). In any case, we prepare the wasserstein loss function that the neural networks are going to use as the metric for minimizing their error. Given that `y_true` will be binary with value -1 for an image coming from the real sample and 1 for a value coming from the generator, this will set up a proper basis for the critic to minimize unto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# luckily for us, the wasserstein loss is very easy to define in practise\n",
    "def wasserstein_loss(y_true, y_pred):\n",
    "    return backend.mean(y_true * y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipConstraint(Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return backend.clip(weights, -self.clip_value, self.clip_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the critic model. To disect a bit the different components within it:\n",
    "\n",
    "* `in_shape(x,y,z)` is referencing the **input shape** that the critic will accept. The $(x,y)$ pair refers to the pixels of a $x\\times y$ image, while $z$ is left to the channel, which in our case is 1 as we simply use greyscaled images but it could be 3 in case that all three primary chromatic sources are deployed.\n",
    "* The weight initialization parameter `init` corresponds to the **prior** $p(z)$ and is needed to generate the stochastic component of the model. WGAN authors recommend using either a uniform or a gaussian distribution, which is what we decide for in here.\n",
    "* We define the **constraints for the weights** as discussed in kernel [3] above.\n",
    "* `LeakyReLU` allows for a small gradient when a specific link between layers should not be active. The slope is controlled by the hyperparameter `alpha`, and should in general be kept small.\n",
    "* The `Sequential` class allows the pipeline between layers. Later on we will also use it to merge the Critic and the Generator parts of the GAN. We use two hidden layers before encoding the outcome into a single value, the critic \"score\" defining how real an image looks.\n",
    "* For details on `Conv2D`, visit the keras documentation: https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D. The 4x4 convolutional window has been chosen based on standard practise. \n",
    "* **Batch normalization** is applied at every layer to re-standarize the outcomes from the previous layer and ensure proper performance. Spectral normalization (see the report) could also be considered.\n",
    "* The activation step (from the last hidden layer to the output) requires of a **linear function** to assign the critic score. This is the default for `Dense(1)`. However, we need to pass an array to the activation layer, so `Flatten()` is used to convert the previous output tensor to the correct shape.\n",
    "* The WGAN paper mentions that the model is unstable under momentum based optimizers, and instead decides to deploy `RMSProp`, so we follow their lead on this decision. The learning rate is set to a small value to ensure convergence, the other main potential issue that the authors discuss on that regard.\n",
    "\n",
    "With this, we define the `define_critic` function for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_critic(in_shape=(80,80,1)):\n",
    "    # weight initialization and constraint\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    const = ClipConstraint(0.005)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # downsize to 40x40\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, \n",
    "                     kernel_constraint=const, input_shape=in_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsize to 20x20\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, \n",
    "                     kernel_constraint=const, input_shape=in_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsize to 10x10\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, \n",
    "                     kernel_constraint=const))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # downsize to 5x5\n",
    "    model.add(Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init, \n",
    "                     kernel_constraint=const))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # preparing and applying linear activation\n",
    "    model.add(Flatten()) # Convert the tensor into an array of values to pass to the activation layer\n",
    "    model.add(Dense(1)) # Activation\n",
    "    # compile model\n",
    "    opt = RMSprop(lr=0.00005) # set the optimization method\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt) # apply the loss metric that we specified before based on the \n",
    "    # approximation to Wasserstein-1 loss proposed.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly for the generator:\n",
    "\n",
    "* We again need a prior set by a gaussian distribution\n",
    "* `n_nodes = 128x5x5`, as we will generate 128 low resolution samples of a $5\\times5$ image given the input. These images are then **upscaled** through successive convolutional layers, doubling the perimeter and squaring the area.\n",
    "* A final **hyperbolic tangent** is used for the activation layer, which will output a $80\\times80$ greyscaled image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_generator(latent_dim):\n",
    "    # weight initialization\n",
    "    init = RandomNormal(stddev=0.02)\n",
    "    # define model\n",
    "    model = Sequential()\n",
    "    # prepare 5x5 image\n",
    "    n_nodes = 128 * 5 * 5\n",
    "    model.add(Dense(n_nodes, kernel_initializer=init, input_dim=latent_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Reshape((5, 5, 128)))\n",
    "    # upsize to 10x10\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsize to 20x20\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsize to 40x40\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # upsize to 80x80\n",
    "    model.add(Conv2DTranspose(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    # output 80x80x1\n",
    "    model.add(Conv2D(1, (5,5), activation='tanh', padding='same', kernel_initializer=init))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was a mouthful! The tough work is done, now we \"*only*\" need to define the rest of the environment where these models will operate. Starting simple, the WGAN as a whole is defined using the `Sequential()` class to assemble the critic and the generator parts together. Given that the critic is trained in its own level of iterations, we impose fixed weights for it on the larger model. This does not affect the weight assigning process of the critic model per se."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the combined generator and critic model, for updating the generator\n",
    "def define_wgan(generator, critic):\n",
    "    # make weights in the critic not trainable when the generator updates\n",
    "    for layer in critic.layers:\n",
    "        if not isinstance(layer, BatchNormalization):\n",
    "            layer.trainable = False\n",
    "    # create the environment to connect both\n",
    "    model = Sequential()\n",
    "    # add generator\n",
    "    model.add(generator)\n",
    "    # add the critic\n",
    "    model.add(critic)\n",
    "    # set the optimization algorithm and ultimate the model's structure\n",
    "    opt = RMSprop(lr=0.00005)\n",
    "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `load_real_samples()` function. The process by which we exactly found out how to define it is detailed in *image_processing.ipynb*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "    X = np.empty([6000,80,80,1])\n",
    "    #X = np.empty([10,103,96,1])\n",
    "    path_to_image = os.listdir(\"Fingerprint images/Real\")\n",
    "    for i in range(6000):\n",
    "        X_i = Image.open(\"Fingerprint images/Real/\"+path_to_image[i]).resize((84,84))\n",
    "        X_i = ImageOps.grayscale(X_i)\n",
    "        X_i = np.asarray(X_i)[1:81,1:81]\n",
    "        X_i = expand_dims(X_i, axis=-1) # expand to 3d, e.g. add channels\n",
    "        X_i = X_i.astype('float32') # convert from ints to floats\n",
    "        X_i[0,:,0] = np.zeros(len(X_i[0,:,0])) + 255\n",
    "        X_i[:,0,0] = np.zeros(len(X_i[:,0,0])) + 255\n",
    "        X_i = -(X_i - 127.5) / 127.5 # scale from [0,255] to [-1,1]\n",
    "        X[i] = X_i\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function to take real samples from all of the image data and label them with $y = -1$ (real image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_real_samples(dataset, n_samples):\n",
    "    # obtain random indexes to sample the real images\n",
    "    ix = randint(0, dataset.shape[0], n_samples)\n",
    "    # sample the real images\n",
    "    X = dataset[ix]\n",
    "    # assign the -1 label to the values coming from the real data\n",
    "    y = -ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the necessary inputs for the generator as a function of the number of samples and the outputs of the critic. Reshape them in an appropiate form to treat later within the generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_latent_points(latent_dim, n_samples):\n",
    "    # get the random initialization values for the latent space\n",
    "    x_input = randn(latent_dim * n_samples)\n",
    "    # reshape. We need to distribute the random coordinates generated before across all the samples\n",
    "    x_input = x_input.reshape(n_samples, latent_dim)\n",
    "    return x_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the previously defined function to generate the priors. Use them as input to create fake images which will be stored in the pertinent parameters of $X$. Label them as $y=1$ (fake data) for the next generation of the discriminator to work unto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_samples(generator, latent_dim, n_samples):\n",
    "    # use the function in kernel[9] to generate the random latent points \n",
    "    x_input = generate_latent_points(latent_dim, n_samples)\n",
    "    # predict outputs\n",
    "    X = generator.predict(x_input)\n",
    "    # assign the 1 label to the fake images that come from the discriminator.\n",
    "    y = ones((n_samples, 1))\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a summary function. What we want is to generate fake samples with our latest WGAN iteration, plot some of them to see how they look like, and store the plot and the model as files for later inspection and use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(step, g_model, latent_dim, n_samples=100):\n",
    "    # use the generator to create fake data\n",
    "    X, _ = generate_fake_samples(g_model, latent_dim, n_samples)\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    X = (X + 1) / 2.0\n",
    "    # plot images\n",
    "    for i in range(4 * 6):\n",
    "        pyplot.subplot(4, 6, 1 + i)\n",
    "        pyplot.axis('off')\n",
    "        pyplot.imshow(X[i, :, :, 0], cmap='gray_r')\n",
    "    # save plot\n",
    "    filename1 = 'generated_plot_%04d.png' % (step+1)\n",
    "    pyplot.savefig(filename1)\n",
    "    pyplot.close()\n",
    "    # save the model\n",
    "    filename2 = 'model_%04d.h5' % (step+1)\n",
    "    g_model.save(filename2)\n",
    "    print('>Saved: %s and %s' % (filename1, filename2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we define the parameters to plot the evolution of the losses for the critic with real and fake data (c1,c2) and for the generator(g), as the model self-optimizes with subsequent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(d1_hist, d2_hist, g_hist):\n",
    "    pyplot.plot(d1_hist, label='crit_real')\n",
    "    pyplot.plot(d2_hist, label='crit_fake')\n",
    "    pyplot.plot(g_hist, label='gen')\n",
    "    pyplot.legend()\n",
    "    pyplot.savefig('plot_line_plot_loss.png')\n",
    "    pyplot.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put everything together to train the model. Some commentary: \n",
    "\n",
    "* Half batches as $\\frac{1}{2}$ corresponds to real images and the other half to the generator's output.\n",
    "* GANs in general are about updating the Discriminator / Critic several times for every round of Generator updating, as in theory there is no risk of making the process fail due to vanishing gradients. \n",
    "* The number of epochs and batch size is arbitrary. Depending on the complexity of image patterns and pixel size, it can take more or less time for the WGAN to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(g_model, c_model, gan_model, dataset, latent_dim, n_epochs=30, n_batch=64, n_critic=10):\n",
    "    # batches per training epoch and training iterations\n",
    "    bat_per_epo = int(dataset.shape[0] / n_batch)\n",
    "    n_steps = bat_per_epo * n_epochs\n",
    "    half_batch = int(n_batch / 2)\n",
    "    # lists for keeping track of loss\n",
    "    c1_hist, c2_hist, g_hist = list(), list(), list()\n",
    "    # manually enumerate epochs\n",
    "    for i in range(n_steps):\n",
    "        # update the critic more than the generator\n",
    "        c1_tmp, c2_tmp = list(), list()\n",
    "        for _ in range(n_critic):\n",
    "            # sample from the data\n",
    "            X_real, y_real = generate_real_samples(dataset, half_batch)\n",
    "            # update critic based on real images\n",
    "            c_loss1 = c_model.train_on_batch(X_real, y_real)\n",
    "            c1_tmp.append(c_loss1)\n",
    "            # use the latest iteration of the generator to create fake images\n",
    "            X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)\n",
    "            # update critic based on fake images\n",
    "            c_loss2 = c_model.train_on_batch(X_fake, y_fake)\n",
    "            c2_tmp.append(c_loss2)\n",
    "        # store critic loss\n",
    "        c1_hist.append(mean(c1_tmp))\n",
    "        c2_hist.append(mean(c2_tmp))\n",
    "        # prepare the latent points that we will introduce in the generator and label the fake images\n",
    "        X_gan = generate_latent_points(latent_dim, n_batch)\n",
    "        y_gan = -ones((n_batch, 1))\n",
    "        # generator actualization based on the discrimination made by the critic\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "        g_hist.append(g_loss)\n",
    "        # summarize loss for current iteration\n",
    "        print('>%d, c1=%.3f, c2=%.3f g=%.3f' % (i+1, c1_hist[-1], c2_hist[-1], g_loss))\n",
    "        # assess the evolution of the WGAN for the current iteration\n",
    "        if (i+1) % bat_per_epo == 0:\n",
    "            summarize_performance(i, g_model, latent_dim)\n",
    "    # once the WGAN has finished the training, we plot the evolution of the Wasserstein-1 measurements\n",
    "    plot_history(c1_hist, c2_hist, g_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the number of latent dimensions that we want to model. Then, we define the critic and generator objects as specified above, and merge them using the `define_wgan(generator, critic)` function. Finally we load the data and make a quick check that it is stored correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 80, 80, 1)\n"
     ]
    }
   ],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 200\n",
    "# define critic and generator, assign latent dimension to the generator.\n",
    "critic = define_critic()\n",
    "generator = define_generator(latent_dim)\n",
    "# define the WGAN object\n",
    "wgan_model = define_wgan(generator, critic)\n",
    "# load image data\n",
    "dataset = load_real_samples()\n",
    "print(dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only thing left to do is to press the red button and enjoy. This launches the training of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(generator, critic, wgan_model, dataset, latent_dim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
